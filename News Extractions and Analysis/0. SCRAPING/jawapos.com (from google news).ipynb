{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impor library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as r\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from urllib.request import Request, urlopen\n",
    "import pandas as pd\n",
    "from time import time, sleep\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inisiasi scrapper google news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GOOGLENEWS_SCRAPPER(object):\n",
    "    def __init__(self, QUERY, SITE, MIN_MONTH, MIN_DATE, MIN_YEAR, MAX_MONTH, MAX_DATE, MAX_YEAR):\n",
    "        self.query = QUERY\n",
    "        self.site = SITE\n",
    "        self.min_month = MIN_MONTH\n",
    "        self.min_date = MIN_DATE\n",
    "        self.min_year = MIN_YEAR\n",
    "        self.max_month = MAX_MONTH\n",
    "        self.max_date = MAX_DATE\n",
    "        self.max_year = MAX_YEAR\n",
    "        \n",
    "    def append_news(self, news, start_page):\n",
    "        news_temp = []\n",
    "        count = 0\n",
    "        URL = \"https://www.google.com/search?q={query}+site:{site}&safe=strict&client=firefox-b-d&tbs=sbd:1,nsd:1,qdr:y&tbm=nws&ei=sH_rX9yDNKLC3LUPguipiAI&start={start_page}&sa=N&filter=0&ved=0ahUKEwjcm6Hp8vPtAhUiIbcAHQJ0CiE4FBDy0wMIhwE&biw=1525&bih=730&dpr=0.9\" # setahun terakhir,urutan tanggal, duplikat\n",
    "        req = Request(URL.format(query = self.query, \n",
    "                                 site = self.site, \n",
    "                                 min_month=self.min_month, min_date=self.min_date, min_year=self.min_year, \n",
    "                                 max_month=self.max_month, max_date=self.max_date, max_year=self.max_year,\n",
    "                                 start_page=start_page\n",
    "                                ),\n",
    "                      headers={'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5)\\\n",
    "            AppleWebKit/537.36 (KHTML, like Gecko) Cafari/537.36'})\n",
    "        webpage = urlopen(req).read()\n",
    "        with r.Session() as c:\n",
    "            soup = bs(webpage, 'html5lib')\n",
    "            for item in soup.find_all('div',attrs={'class':'kCrYT'}):\n",
    "                if count%2==0:\n",
    "                    title = item.select('div.BNeawe.vvjwJb.AP7Wnd')[0].text\n",
    "                    site = item.select('div.BNeawe.UPmit.AP7Wnd')[0].text\n",
    "                    link = item.find('a',attrs={'href':True})['href'].split('/url?q=')[1]\n",
    "                else:\n",
    "                    time = item.select('span.r0bn4c.rQMQod')[0].text\n",
    "                    text = item.select('div.BNeawe.s3v9rd.AP7Wnd')[0].text.split(' Â· ')[1]\n",
    "                    news.append([link,time,site,title,text])\n",
    "                    news_temp.append([link,time,site,title,text])\n",
    "                    link = \"\" ; title = \"\"; site = \"\"; time = \"\"; text = \"\"\n",
    "                count+=1\n",
    "        return news, news_temp\n",
    "    \n",
    "    def scrap(self):\n",
    "        self.news = []\n",
    "        self.news_temp = []\n",
    "        \n",
    "        start_page = 0\n",
    "        append_again = True\n",
    "        i = 0;\n",
    "        \n",
    "        while append_again:\n",
    "            self.news, self.news_temp= self.append_news(self.news, start_page)\n",
    "            i+=1; print('collecting news: page {}'.format(i))\n",
    "            sleep(5)\n",
    "            if len(self.news_temp)<10:\n",
    "                append_again = False\n",
    "            else:\n",
    "                start_page +=10\n",
    "        print('Result:{} news items.'.format(i*10+len(self.news_temp)))\n",
    "        return self.news\n",
    "    \n",
    "    def scrap_toPandas(self):\n",
    "        self.data = pd.DataFrame(self.news, columns = ['Link','Time','Site','Title','Text'])\n",
    "        return self.data\n",
    "        \n",
    "    def scrap_save(self):\n",
    "        FOLDER_DIR = os.path.join(os.getcwd(), \"googlenews\")\n",
    "        if not os.path.exists(FOLDER_DIR):\n",
    "            os.mkdir(FOLDER_DIR)\n",
    "        FILENAME = \"{}_{}_{}-{}-{}_{}-{}-{}.csv\".format(self.query, self.site, self.min_month, self.min_date, self.min_year, self.max_month, self.max_date, self.max_year)\n",
    "        FILE_DIR = os.path.join(FOLDER_DIR, FILENAME)\n",
    "        self.data.to_csv(FILE_DIR)\n",
    "        print('Data saved to:{}'.format(FILE_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Menjalankan scrapper google news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrap:radargresik.jawapos.com\n",
      "collecting news: page 1\n",
      "collecting news: page 2\n",
      "collecting news: page 3\n",
      "collecting news: page 4\n",
      "collecting news: page 5\n",
      "collecting news: page 6\n",
      "collecting news: page 7\n",
      "collecting news: page 8\n",
      "collecting news: page 9\n",
      "collecting news: page 10\n",
      "collecting news: page 11\n",
      "Result:115 news items.\n",
      "Data saved to:C:\\Users\\Dwi Nissa\\Skripsi\\CARI METODE\\SCRAPPING\\googlenews\\kecelakaan_radargresik.jawapos.com_1-1-2020_12-30-2020.csv\n"
     ]
    }
   ],
   "source": [
    "# Set Values\n",
    "QUERY = \"kecelakaan\"\n",
    "SITE = [\"radargresik.jawapos.com\"]\n",
    "MIN_MONTH = 1; MIN_DATE = 1; MIN_YEAR = 2020; MAX_MONTH = 12; MAX_DATE = 31; MAX_YEAR = 2020\n",
    "n = 2; wait = 1\n",
    "for i in range(len(SITE)):\n",
    "    print('Scrap:{}'.format(SITE[i]))\n",
    "    scrapper = GOOGLENEWS_SCRAPPER(QUERY, SITE[i], MIN_MONTH, MIN_DATE, MIN_YEAR, MAX_MONTH, MAX_DATE, MAX_YEAR)\n",
    "    success = False\n",
    "    while not success:\n",
    "        try:\n",
    "            news = scrapper.scrap()\n",
    "            success = True\n",
    "            scrapper.scrap_toPandas()\n",
    "            scrapper.scrap_save()\n",
    "        except Exception as e:\n",
    "            wait = wait * n;\n",
    "            print('[%s] Error! Waiting %s secs and re-trying...' % (e, wait))\n",
    "            sleep(wait)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
